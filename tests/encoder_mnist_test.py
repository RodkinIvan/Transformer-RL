# -*- coding: utf-8 -*-
"""CoBERL-Encoder Test with MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lVJjTKhXI2Vxit9g75ODDBpO8Qp5p9UN
"""

import torch
import numpy as np
import random
import pandas as pd

def fixSeeds():
  # To make random weights in NN consistent from launch to launch on this device
  # To actually distinguish an improvement
  random.seed(0)
  np.random.seed(0)
  torch.manual_seed(0)
  torch.cuda.manual_seed(0)
  torch.backends.cudnn.deterministic = True

fixSeeds()

import torchvision.datasets
MNIST_train = torchvision.datasets.MNIST('./', download=True, train=True)
MNIST_test = torchvision.datasets.MNIST('./', download=True, train=False)

X_train = MNIST_train.train_data
y_train = MNIST_train.train_labels
X_test = MNIST_test.test_data
y_test = MNIST_test.test_labels

X_train.dtype, y_train.dtype

X_train = X_train.float()
X_test = X_test.float()

X_train.shape, X_test.shape

y_train.shape, y_test.shape

import matplotlib.pyplot as plt
plt.imshow(X_train[0, :, :])
plt.show()
y_train[0]

torch.cuda.is_available()

!nvidia-smi

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# import torch.nn as nn
# class MNISTNet(nn.Module):
#   def __init__(self, input_neurons, hidden_neurons):
#     super(MNISTNet, self).__init__()
#     self.input_neurons = input_neurons
#     self.hidden_neurons = hidden_neurons

#     self.fc1 = nn.Linear(input_neurons, hidden_neurons)
#     self.al1 = nn.Sigmoid()
#     self.fc2 = nn.Linear(hidden_neurons, 10)
  
#   def forward(self, x):
#     x = self.fc1(x)
#     x = self.al1(x)
#     x = self.fc2(x)
#     return x
# mnistNet = MNISTNet(28*28, 100)

from encoder import ResNet47

mnistNet = ResNet47(10, channels=1)

mnistNet = mnistNet.to(device)
mnistNet.parameters()

# X_train = X_train.reshape([-1, 28*28])
# X_test = X_test.reshape([-1, 28*28])

X_train = torch.reshape(X_train, shape=(60000, 1, 28, 28))
X_test = torch.reshape(X_test, shape=(10000, 1, 28, 28))

X_train.shape

X_train.shape, X_test.shape

loss = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mnistNet.parameters(), lr=1e-3)



batch_size = 4
# X_train = X_train.to(device)
# y_train = X_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

test_accuracy_history = []
test_loss_history = []
train_loss_history = []


for epoch in range(5):
  permutation = np.random.permutation(len(X_train))
  loss_value = 0
  for i in range(0, len(X_train), batch_size):
    optimizer.zero_grad()
    X_batch = X_train[permutation[i : i+batch_size]].to(device)
    y_batch = y_train[permutation[i : i+batch_size]].to(device)
    # print(X_batch.shape)
    prediction = mnistNet.forward(X_batch)
    loss_value = loss(prediction, y_batch)
    loss_value.backward()
    optimizer.step()

  train_loss_history.append(loss_value)
  test_prediction = mnistNet.forward(X_test)
  accuracy = (test_prediction.argmax(dim=1) == y_test).float().mean()
  print(accuracy)
  test_loss_history.append(loss(test_prediction, y_test))
  test_accuracy_history.append(accuracy)



plt.plot(train_loss_history)
plt.plot(test_loss_history)



