# -*- coding: utf-8 -*-
"""CoBERL-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/144tOCZVfDs31GTa72p-_BGappRR63LH9
"""


import torch
import torch.nn as nn
from torch.nn import KLDivLoss
import torch.nn.functional as F
from Transformer_RL.model.encoder import VisualEncoder

# !rm -r Transformer_RL/
# !git clone -b dev https://github.com/RodkinIvan/Transformer-RL

# Commented out IPython magic to ensure Python compatibility.
# %cd Transformer_RL/

# !ls

from Transformer_RL.GTrXL.gtrxl import GTrXL


class ValueNetwork(nn.Module):
    def __init__(self, input_dim, out_dim):
        super(ValueNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, 512)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(512, 30)
        self.fc3 = nn.Linear(30, out_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        # x = self.relu(x)
        return x



class CoBERL(nn.Module):
    def __init__(self, H,
                       input_dim=512,
                       head_dim=64,
                       embedding_dim=512,
                       head_num=8,
                       mlp_num=2,
                       layer_num=8,
                       memory_len=64,
                       activation=nn.GELU(),
                       out_dim = 4,
                       constrastive_loss=None, contrastive_mask_rate=None):
        super(CoBERL, self).__init__()
        self.embedding_dim = embedding_dim
        self.encoder = VisualEncoder()
        self.input_dim = input_dim
        self.out_dim = out_dim
        self.gtrxl = GTrXL(input_dim=input_dim,
                           head_dim=head_dim,
                           embedding_dim=embedding_dim,
                           head_num=head_num,
                           mlp_num=mlp_num,
                           layer_num=layer_num,
                           memory_len=memory_len,
                           activation=activation
                           )

        self.critic_function = nn.Linear(embedding_dim, self.input_dim)
        self.gru = nn.GRU(input_size=self.embedding_dim, hidden_size=self.embedding_dim, num_layers=1, bias=True)
        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.embedding_dim, num_layers=1)
        self.v_head = ValueNetwork(2*out_dim, out_dim)

        self.H = H

    @staticmethod
    def create_masks_targets(input, perc=0.15, token=0):
        mask = input.ge(perc)
        masked = input * mask
        return masked, input, mask

    def compute_aux_loss(self, input1, input2, mask_ext):
        # print(input1.shape)
        batch_size, seq_dim, feat_dim = input1.shape
        input1 = torch.reshape(input1, shape=(-1, feat_dim))
        input2 = torch.reshape(input2, shape=(-1, feat_dim))

        input1 = F.normalize(input1, p=2)  # l2_normalize(input1, axis=-1)
        input2 = F.normalize(input2, p=2)  # l2_normalize(input2, axis=-1)

        # Compute labels index
        labels_idx = torch.arange(start=0, end=input1.shape[0])
        # labels_idx = labels_idx.astype(jnp.int32)
        # Compute pseudo-labels for contrastive loss.
        labels = F.one_hot(labels_idx, input1.shape[0] * 2).to(self.H.device)
        # Mask out the same image pair.
        mask = F.one_hot(labels_idx, input1.shape[0]).to(self.H.device)

        # Compute logits.
        logits_11 = torch.matmul(input1, input1.T)
        logits_22 = torch.matmul(input2, input2.T)
        logits_12 = torch.matmul(input1, input2.T)
        logits_21 = torch.matmul(input2, input1.T)

        # Calculate invariance penalty.
        kl_div = KLDivLoss(reduction="batchmean")
        inv_penalty = kl_div(logits_11.detach(), logits_22)
        inv_penalty += kl_div(logits_12.detach(), logits_22)
        inv_penalty += kl_div(logits_21.detach(), logits_11)
        inv_penalty += kl_div(logits_12.detach(), logits_21)
        inv_penalty = inv_penalty / 4.  # [B * T]

        logits_11 = logits_11 - mask * 1e9
        logits_22 = logits_22 - mask * 1e9

        logits_1211 = torch.concat((logits_12, logits_11))
        logits_2122 = torch.concat((logits_21, logits_22))

        loss_12 = -torch.sum(labels * F.log_softmax(logits_1211, dim=-1))
        loss_21 = -torch.sum(labels * F.log_softmax(logits_2122, dim=-1))
        # print(loss_12.shape, mask_ext.shape)
        loss = torch.reshape(loss_12 + loss_21, [batch_size, seq_dim]) * mask_ext
        inv_penalty = torch.reshape(inv_penalty, [batch_size, seq_dim]) * mask_ext

        loss = torch.mean(loss + inv_penalty)

        return loss

    def forward(self, sampled_batch, previous_rewards=None, encoded_actions=None):
        # The previous reward and one-hot encoded action are concatenated and projected
        # with a linear layer into a 64-dimensional vector

        # encoded_actions = F.one_hot(sampled_batch.long())
        # prev_reward_action = torch.rand(size=(1, 64)) if None in [previous_rewards, encoded_actions] else torch.concat(
        #     (previous_rewards, encoded_actions))

        #y = self.encoder(sampled_batch, prev_reward_action).reshape((1,1,512))
        y = sampled_batch
        # print("fully encoded", y.shape)

        transformer_inputs, transformer_targets, mask = self.create_masks_targets(y, perc=0.15, token=0)
        # print('transformer_inputs', transformer_inputs.shape, transformer_targets.shape, mask.shape)
        output_transformer_contrastive = self.gtrxl(transformer_inputs.reshape((1, 1, self.input_dim)))['logit']

        output_transformer_contrastive = self.critic_function(output_transformer_contrastive)
        transformer_targets = self.critic_function(transformer_targets)

        contrastive_loss = self.compute_aux_loss(output_transformer_contrastive,
                                                 transformer_targets, mask)
        x = self.gtrxl(transformer_inputs.reshape((1, 1, self.input_dim)))['logit']

        # x = self.gtrxl(y)['logit'].reshape((1, 512))
        y = y.reshape((1, 1, self.input_dim))
        # print("x y ", x.shape, y.shape)
        z, h_n = self.gru(y, x)
        # print("GRU", z.shape)
        out, (h_n, c_n) = self.lstm(z)
        # print('lstm', out.shape, y.shape)
        v_input = torch.concat((out, y), dim=1).flatten()
        # print('concat', v_input.shape)

        value_estimation = self.v_head(v_input)
        rl_loss = 0  # compute_rl_loss(value_estimation, extra_args)
        total_loss = contrastive_loss  # + rl_loss

        return value_estimation, total_loss


if __name__ == "__main__":
  model = CoBERL()
  input = torch.rand(size=(4, 3, 240, 240))
  v_estimation, loss = model(input)
  loss.backward()
  print(v_estimation.shape)
